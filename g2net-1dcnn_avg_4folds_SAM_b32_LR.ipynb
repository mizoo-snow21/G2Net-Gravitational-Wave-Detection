{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Basically a 1D CNN starter with bandpass. Filter size hard-coded from [https://www.kaggle.com/kit716/grav-wave-detection](https://www.kaggle.com/kit716/grav-wave-detection) which uses the simple architecture from https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103 \n",
    "- Added inference to @hidehisaarai1213 's PyTorch starter, iteration order changed from Y.Nakama's pipeline: \"iter on loader first then load model\" to \"load model first then iter the loader\"\n",
    "- Version 3: average pool+ELU\n",
    "- Version 4: max pool+SiLU\n",
    "- Version 5: Generalized Mean pooling: a trainable L^p mean per channel (using ideas from Lebesgue measurable spaces) pooling added per the comments from @hannes82:\n",
    "   $$\\textbf{e} = \\left[\\left(\\frac{1}{|\\Omega|}\\sum_{u\\in{\\Omega}}x^{p}_{cu}\\right)^{\\frac{1}{p}}\\right]_{c=1,\\cdots,C} $$\n",
    "\n",
    "## Reference\n",
    "- pipeline: [Y.Nakama's notebook](https://www.kaggle.com/yasufuminakama/g2net-efficientnet-b7-baseline-training).\n",
    "- dataset: @hidehisaarai1213 https://www.kaggle.com/hidehisaarai1213/g2net-read-from-tfrecord-train-with-pytorch\n",
    "- 1d CNN modified from https://www.kaggle.com/kit716/grav-wave-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014815,
     "end_time": "2021-05-11T16:05:42.265815",
     "exception": false,
     "start_time": "2021-05-11T16:05:42.251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:53:12.680337Z",
     "iopub.status.busy": "2021-09-18T04:53:12.679692Z",
     "iopub.status.idle": "2021-09-18T04:53:19.271353Z",
     "shell.execute_reply": "2021-09-18T04:53:19.270367Z",
     "shell.execute_reply.started": "2021-09-18T04:53:12.680234Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import tensorflow as tf  # for reading TFRecord Dataset\n",
    "import tensorflow_datasets as tfds  # for making tf.data.Dataset to return numpy arrays\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:53:19.27313Z",
     "iopub.status.busy": "2021-09-18T04:53:19.272778Z",
     "iopub.status.idle": "2021-09-18T04:53:19.317374Z",
     "shell.execute_reply": "2021-09-18T04:53:19.316144Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.273097Z"
    }
   },
   "outputs": [],
   "source": [
    "SAVEDIR = Path(\"./\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:55:10.240291Z",
     "iopub.status.busy": "2021-09-18T04:55:10.239954Z",
     "iopub.status.idle": "2021-09-18T04:55:10.247133Z",
     "shell.execute_reply": "2021-09-18T04:55:10.246079Z",
     "shell.execute_reply.started": "2021-09-18T04:55:10.240254Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    print_freq = 2500\n",
    "    num_workers = 0\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    #scheduler = 'ReduceLROnPlateau'\n",
    "    model_name = \"1dcnn\"\n",
    "    epochs = 8\n",
    "    T_max = 8\n",
    "    lr = 5e-4\n",
    "    min_lr = 1e-7\n",
    "    batch_size = 32\n",
    "    val_batch_size = 64\n",
    "    weight_decay = 1e-7\n",
    "    gradient_accumulation_steps = 1\n",
    "    max_grad_norm = 1000\n",
    "    factor = 0.2\n",
    "    patience = 1\n",
    "    eps = 1e-7\n",
    "    seed = 42\n",
    "    target_size = 1\n",
    "    target_col = \"target\"\n",
    "    n_fold = 4\n",
    "    trn_fold = [0, 1, 2, 3]\n",
    "    train = True\n",
    "    bandpass_params = dict(lf=30, \n",
    "                           hf=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028465,
     "end_time": "2021-05-11T16:05:49.544956",
     "exception": false,
     "start_time": "2021-05-11T16:05:49.516491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:53:19.3298Z",
     "iopub.status.busy": "2021-09-18T04:53:19.329323Z",
     "iopub.status.idle": "2021-09-18T04:53:19.341319Z",
     "shell.execute_reply": "2021-09-18T04:53:19.340456Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.329762Z"
    },
    "papermill": {
     "duration": 0.041144,
     "end_time": "2021-05-11T16:05:49.614272",
     "exception": false,
     "start_time": "2021-05-11T16:05:49.573128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=SAVEDIR / 'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch(seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord Loader\n",
    "\n",
    "This is the heart of this notebook. Instead of using PyTorch's Dataset and DataLoader, here I define custom Loader that reads samples from TFRecords.\n",
    "\n",
    "FYI, there's a library that does the same thing, but its implementation is not optimized, so it's slower.\n",
    "\n",
    "https://github.com/vahidk/tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:53:19.343051Z",
     "iopub.status.busy": "2021-09-18T04:53:19.342657Z",
     "iopub.status.idle": "2021-09-18T04:53:20.919516Z",
     "shell.execute_reply": "2021-09-18T04:53:20.918379Z",
     "shell.execute_reply.started": "2021-09-18T04:53:19.343017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_files:  20\n"
     ]
    }
   ],
   "source": [
    "path = f\"train\"\n",
    "            \n",
    "all_files = []\n",
    "all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/train*.tfrecords\"))))\n",
    "    \n",
    "print(\"train_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:41.249881Z",
     "iopub.status.busy": "2021-09-18T04:54:41.24955Z",
     "iopub.status.idle": "2021-09-18T04:54:41.254721Z",
     "shell.execute_reply": "2021-09-18T04:54:41.253636Z",
     "shell.execute_reply.started": "2021-09-18T04:54:41.249849Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_data_items(fileids, train=True):\n",
    "    \"\"\"\n",
    "    Count the number of samples.\n",
    "    Each of the TFRecord datasets is designed to contain 28000 samples for train\n",
    "    22500 for test.\n",
    "    \"\"\"\n",
    "    sizes = 28000 if train else 22500\n",
    "    return len(fileids) * sizes\n",
    "\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandpass\n",
    "\n",
    "Modified from various notebooks and https://www.kaggle.com/c/g2net-gravitational-wave-detection/discussion/261721#1458564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:41.876507Z",
     "iopub.status.busy": "2021-09-18T04:54:41.876174Z",
     "iopub.status.idle": "2021-09-18T04:54:41.883759Z",
     "shell.execute_reply": "2021-09-18T04:54:41.882845Z",
     "shell.execute_reply.started": "2021-09-18T04:54:41.876474Z"
    }
   },
   "outputs": [],
   "source": [
    "def bandpass(x, lf=20, hf=500, order=8, sr=2048):\n",
    "    '''\n",
    "    Cell 33 of https://www.gw-openscience.org/LVT151012data/LOSC_Event_tutorial_LVT151012.html\n",
    "    https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
    "    '''\n",
    "    sos = signal.butter(order, [lf, hf], btype=\"bandpass\", output=\"sos\", fs=sr)\n",
    "    normalization = np.sqrt((hf - lf) / (sr / 2))\n",
    "    window = signal.tukey(4096, 0.1)\n",
    "    if x.ndim ==2:\n",
    "        x *= window\n",
    "        for i in range(3):\n",
    "            x[i] = signal.sosfilt(sos, x[i]) * normalization\n",
    "    elif x.ndim == 3: # batch\n",
    "        for i in range(x.shape[0]):\n",
    "            x[i] *= window\n",
    "            for j in range(3):\n",
    "                x[i, j] = signal.sosfilt(sos, x[i, j]) * normalization\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:42.282062Z",
     "iopub.status.busy": "2021-09-18T04:54:42.281694Z",
     "iopub.status.idle": "2021-09-18T04:54:42.294919Z",
     "shell.execute_reply": "2021-09-18T04:54:42.294048Z",
     "shell.execute_reply.started": "2021-09-18T04:54:42.282028Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_wave(wave):\n",
    "    wave = tf.reshape(tf.io.decode_raw(wave, tf.float64), (3, 4096))\n",
    "    normalized_waves = []\n",
    "    scaling = tf.constant([1.5e-20, 1.5e-20, 0.5e-20], dtype=tf.float64)\n",
    "    for i in range(3):\n",
    "#         normalized_wave = wave[i] / tf.math.reduce_max(wave[i])\n",
    "        normalized_wave = wave[i] / scaling[i]\n",
    "        normalized_waves.append(normalized_wave)\n",
    "    wave = tf.stack(normalized_waves, axis=0)\n",
    "    wave = tf.cast(wave, tf.float32)\n",
    "    return wave\n",
    "\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), tf.reshape(tf.cast(example[\"target\"], tf.float32), [1]), example[\"wave_id\"]\n",
    "\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_id):\n",
    "    tfrec_format = {\n",
    "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, tfrec_format)\n",
    "    return prepare_wave(example[\"wave\"]), example[\"wave_id\"] if return_image_id else 0\n",
    "\n",
    "\n",
    "def get_dataset(files, batch_size=16, repeat=False, cache=False, \n",
    "                shuffle=False, labeled=True, return_image_ids=True):\n",
    "    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO, compression_type=\"GZIP\")\n",
    "    if cache:\n",
    "        # You'll need around 15GB RAM if you'd like to cache val dataset, and 50~60GB RAM for train dataset.\n",
    "        ds = ds.cache()\n",
    "\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024 * 2)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "\n",
    "    if labeled:\n",
    "        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), num_parallel_calls=AUTO)\n",
    "\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return tfds.as_numpy(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:42.88264Z",
     "iopub.status.busy": "2021-09-18T04:54:42.882348Z",
     "iopub.status.idle": "2021-09-18T04:54:42.890621Z",
     "shell.execute_reply": "2021-09-18T04:54:42.889624Z",
     "shell.execute_reply.started": "2021-09-18T04:54:42.882612Z"
    }
   },
   "outputs": [],
   "source": [
    "class TFRecordDataLoader:\n",
    "    def __init__(self, files, batch_size=32, cache=False, train=True, \n",
    "                              repeat=False, shuffle=False, labeled=True, \n",
    "                              return_image_ids=True):\n",
    "        self.ds = get_dataset(\n",
    "            files, \n",
    "            batch_size=batch_size,\n",
    "            cache=cache,\n",
    "            repeat=repeat,\n",
    "            shuffle=shuffle,\n",
    "            labeled=labeled,\n",
    "            return_image_ids=return_image_ids)\n",
    "        \n",
    "        self.num_examples = count_data_items(files, labeled)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.labeled = labeled\n",
    "        self.return_image_ids = return_image_ids\n",
    "        self._iterator = None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self._iterator is None:\n",
    "            self._iterator = iter(self.ds)\n",
    "        else:\n",
    "            self._reset()\n",
    "        return self._iterator\n",
    "\n",
    "    def _reset(self):\n",
    "        self._iterator = iter(self.ds)\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = next(self._iterator)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        n_batches = self.num_examples // self.batch_size\n",
    "        if self.num_examples % self.batch_size == 0:\n",
    "            return n_batches\n",
    "        else:\n",
    "            return n_batches + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029891,
     "end_time": "2021-05-11T16:05:50.085695",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.055804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:43.867595Z",
     "iopub.status.busy": "2021-09-18T04:54:43.867253Z",
     "iopub.status.idle": "2021-09-18T04:54:43.874782Z",
     "shell.execute_reply": "2021-09-18T04:54:43.873944Z",
     "shell.execute_reply.started": "2021-09-18T04:54:43.867562Z"
    }
   },
   "outputs": [],
   "source": [
    "class GeM(nn.Module):\n",
    "    '''\n",
    "    Code modified from the 2d code in\n",
    "    https://amaarora.github.io/2020/08/30/gempool.html\n",
    "    '''\n",
    "    def __init__(self, kernel_size=8, p=3, eps=1e-6):\n",
    "        super(GeM,self).__init__()\n",
    "        self.p = nn.Parameter(torch.ones(1)*p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "    def gem(self, x, p=3, eps=1e-6):\n",
    "        return F.max_pool1d(x.clamp(min=eps).pow(p), self.kernel_size).pow(1./p)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "                ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.middle_features = hidden_dim\n",
    "        self.W = nn.Linear(in_features, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "        score = self.V(att)\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:54:44.958743Z",
     "iopub.status.busy": "2021-09-18T04:54:44.958423Z",
     "iopub.status.idle": "2021-09-18T04:54:44.971206Z",
     "shell.execute_reply": "2021-09-18T04:54:44.970268Z",
     "shell.execute_reply.started": "2021-09-18T04:54:44.958711Z"
    },
    "papermill": {
     "duration": 0.037539,
     "end_time": "2021-05-11T16:05:50.153617",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.116078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN1d(nn.Module):\n",
    "    \"\"\"1D convolutional neural network. Classifier of the gravitational waves.\n",
    "    Architecture from there https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.120.141103\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(3, 64, kernel_size=64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=32),\n",
    "            nn.AvgPool1d(kernel_size=8),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=32),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv1d(128, 128, kernel_size=16),\n",
    "            nn.AvgPool1d(kernel_size=6),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=16),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.cnn6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=16),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256 * 11, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        self.debug = debug\n",
    "\n",
    "    def forward(self, x, pos=None):\n",
    "        x = self.cnn1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = self.cnn3(x)\n",
    "        x = self.cnn4(x)\n",
    "        x = self.cnn5(x)\n",
    "        x = self.cnn6(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029417,
     "end_time": "2021-05-11T16:05:50.211711",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.182294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:55:16.062351Z",
     "iopub.status.busy": "2021-09-18T04:55:16.062023Z",
     "iopub.status.idle": "2021-09-18T04:55:16.070271Z",
     "shell.execute_reply": "2021-09-18T04:55:16.069322Z",
     "shell.execute_reply.started": "2021-09-18T04:55:16.062321Z"
    }
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def max_memory_allocated():\n",
    "    MB = 1024.0 * 1024.0\n",
    "    mem = torch.cuda.max_memory_allocated() / MB\n",
    "    return f\"{mem:.0f} MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "        \n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:55:16.549023Z",
     "iopub.status.busy": "2021-09-18T04:55:16.548682Z",
     "iopub.status.idle": "2021-09-18T04:55:16.568585Z",
     "shell.execute_reply": "2021-09-18T04:55:16.567463Z",
     "shell.execute_reply.started": "2021-09-18T04:55:16.548989Z"
    },
    "papermill": {
     "duration": 0.053284,
     "end_time": "2021-05-11T16:05:50.294128",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.240844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(files, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    train_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size, \n",
    "        shuffle=True)\n",
    "    for step, d in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            \"\"\"\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\"\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            criterion(model(x).view(-1), labels.view(-1)).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('Epoch: [{0}/{1}][{2}/{3}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.6f}  '\n",
    "                  'Elapsed: {remain:s} '\n",
    "                  'Max mem: {mem:s}'\n",
    "                  .format(\n",
    "                   epoch+1, CFG.epochs, step, len(train_loader),\n",
    "                   loss=losses,\n",
    "                   grad_norm=grad_norm,\n",
    "                   lr=scheduler.get_last_lr()[0],\n",
    "                   remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                   mem=max_memory_allocated()))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(files, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    filenames = []\n",
    "    targets = []\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    valid_loader = TFRecordDataLoader(\n",
    "        files, batch_size=CFG.batch_size * 2, shuffle=False)\n",
    "    for step, d in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        targets.extend(d[1].reshape(-1).tolist())\n",
    "        filenames.extend([f.decode(\"UTF-8\") for f in d[2]])\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "        labels = torch.from_numpy(d[1]).to(device)\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        loss = criterion(y_preds.view(-1), labels.view(-1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0:\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds).reshape(-1)\n",
    "    return losses.avg, predictions, np.array(targets), np.array(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030241,
     "end_time": "2021-05-11T16:05:50.353874",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.323633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-18T04:55:17.026632Z",
     "iopub.status.busy": "2021-09-18T04:55:17.026315Z",
     "iopub.status.idle": "2021-09-18T04:55:17.04195Z",
     "shell.execute_reply": "2021-09-18T04:55:17.041071Z",
     "shell.execute_reply.started": "2021-09-18T04:55:17.026601Z"
    },
    "papermill": {
     "duration": 0.049518,
     "end_time": "2021-05-11T16:05:50.433341",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.383823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(train_tfrecords: np.ndarray, val_tfrecords: np.ndarray, fold: int):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                             mode='max', \n",
    "                                                             factor=CFG.factor, \n",
    "                                                             patience=CFG.patience, \n",
    "                                                             verbose=True, \n",
    "                                                             eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                             T_max=CFG.T_max, \n",
    "                                                             eta_min=CFG.min_lr, \n",
    "                                                             last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                       T_0=CFG.T_0, \n",
    "                                                                       T_mult=1, \n",
    "                                                                       eta_min=CFG.min_lr, \n",
    "                                                                       last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CNN1d()\n",
    "    model.to(device)\n",
    "    \n",
    "    base_optimizer = optim.Adam\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(\"\\n\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_tfrecords, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds, targets, files = valid_fn(val_tfrecords, model, criterion, device)\n",
    "        valid_result_df = pd.DataFrame({\"target\": targets, \"preds\": preds, \"id\": files})\n",
    "        \n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, optim.lr_scheduler.CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(targets, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        SAVEDIR / f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    valid_result_df[\"preds\"] = torch.load(SAVEDIR / f\"{CFG.model_name}_fold{fold}_best_loss.pth\",\n",
    "                                          map_location=\"cpu\")[\"preds\"]\n",
    "\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T04:55:17.504337Z",
     "iopub.status.busy": "2021-09-18T04:55:17.50407Z",
     "iopub.status.idle": "2021-09-18T04:55:37.577535Z",
     "shell.execute_reply": "2021-09-18T04:55:37.575158Z",
     "shell.execute_reply.started": "2021-09-18T04:55:17.504312Z"
    },
    "papermill": {
     "duration": 0.038415,
     "end_time": "2021-05-11T16:05:50.501397",
     "exception": false,
     "start_time": "2021-05-11T16:05:50.462982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-27 05:15:05.751092: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.753590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.754295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.756065: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-27 05:15:05.756411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.757174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.757840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.759030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.759741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.760368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-09-27 05:15:05.760971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13740 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
      "2021-09-27 05:15:06.267577: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/8][0/13125] Loss: 0.6947(0.6947) Grad: 12.4073  LR: 0.000500  Elapsed: 0m 2s (remain 458m 36s) Max mem: 222 MB\n",
      "Epoch: [1/8][2500/13125] Loss: 0.4307(0.4992) Grad: 0.2999  LR: 0.000500  Elapsed: 3m 2s (remain 12m 55s) Max mem: 242 MB\n",
      "Epoch: [1/8][5000/13125] Loss: 0.4767(0.4808) Grad: 0.2594  LR: 0.000500  Elapsed: 6m 3s (remain 9m 51s) Max mem: 242 MB\n",
      "Epoch: [1/8][7500/13125] Loss: 0.2951(0.4712) Grad: 0.3236  LR: 0.000500  Elapsed: 9m 5s (remain 6m 48s) Max mem: 242 MB\n",
      "Epoch: [1/8][10000/13125] Loss: 0.3775(0.4641) Grad: 0.2439  LR: 0.000500  Elapsed: 12m 6s (remain 3m 46s) Max mem: 242 MB\n",
      "Epoch: [1/8][12500/13125] Loss: 0.4355(0.4590) Grad: 0.2190  LR: 0.000500  Elapsed: 15m 7s (remain 0m 45s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.202 (0.202) Elapsed 0m 0s (remain 9m 2s) Loss: 0.4964(0.4964) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "Epoch 1 - avg_train_loss: 0.4579  avg_val_loss: 0.4304  time: 1087s\n",
      "Epoch 1 - Score: 0.8658\n",
      "Epoch 1 - Save Best Score: 0.8658 Model\n",
      "Epoch 1 - Save Best Loss: 0.4304 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [2/8][0/13125] Loss: 0.4112(0.4112) Grad: 0.2912  LR: 0.000481  Elapsed: 0m 1s (remain 259m 41s) Max mem: 242 MB\n",
      "Epoch: [2/8][2500/13125] Loss: 0.4368(0.4299) Grad: 0.2567  LR: 0.000481  Elapsed: 3m 4s (remain 13m 1s) Max mem: 242 MB\n",
      "Epoch: [2/8][5000/13125] Loss: 0.3523(0.4283) Grad: 0.2720  LR: 0.000481  Elapsed: 6m 5s (remain 9m 53s) Max mem: 242 MB\n",
      "Epoch: [2/8][7500/13125] Loss: 0.4617(0.4286) Grad: 0.1784  LR: 0.000481  Elapsed: 9m 6s (remain 6m 49s) Max mem: 242 MB\n",
      "Epoch: [2/8][10000/13125] Loss: 0.4410(0.4274) Grad: 0.2926  LR: 0.000481  Elapsed: 12m 8s (remain 3m 47s) Max mem: 242 MB\n",
      "Epoch: [2/8][12500/13125] Loss: 0.4977(0.4268) Grad: 0.2279  LR: 0.000481  Elapsed: 15m 9s (remain 0m 45s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.205 (0.205) Elapsed 0m 0s (remain 9m 32s) Loss: 0.4842(0.4842) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.4267  avg_val_loss: 0.4195  time: 1088s\n",
      "Epoch 2 - Score: 0.8693\n",
      "Epoch 2 - Save Best Score: 0.8693 Model\n",
      "Epoch 2 - Save Best Loss: 0.4195 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [3/8][0/13125] Loss: 0.3702(0.3702) Grad: 0.1821  LR: 0.000427  Elapsed: 0m 1s (remain 262m 42s) Max mem: 242 MB\n",
      "Epoch: [3/8][2500/13125] Loss: 0.2478(0.4185) Grad: 0.2368  LR: 0.000427  Elapsed: 3m 2s (remain 12m 54s) Max mem: 242 MB\n",
      "Epoch: [3/8][5000/13125] Loss: 0.4163(0.4184) Grad: 0.2713  LR: 0.000427  Elapsed: 6m 4s (remain 9m 51s) Max mem: 242 MB\n",
      "Epoch: [3/8][7500/13125] Loss: 0.2918(0.4189) Grad: 0.2858  LR: 0.000427  Elapsed: 9m 6s (remain 6m 49s) Max mem: 242 MB\n",
      "Epoch: [3/8][10000/13125] Loss: 0.3860(0.4181) Grad: 0.1840  LR: 0.000427  Elapsed: 12m 7s (remain 3m 47s) Max mem: 242 MB\n",
      "Epoch: [3/8][12500/13125] Loss: 0.3936(0.4182) Grad: 0.1249  LR: 0.000427  Elapsed: 15m 7s (remain 0m 45s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.181 (0.181) Elapsed 0m 0s (remain 8m 42s) Loss: 0.4695(0.4695) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.4181  avg_val_loss: 0.4105  time: 1082s\n",
      "Epoch 3 - Score: 0.8724\n",
      "Epoch 3 - Save Best Score: 0.8724 Model\n",
      "Epoch 3 - Save Best Loss: 0.4105 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [4/8][0/13125] Loss: 0.4258(0.4258) Grad: 0.1743  LR: 0.000346  Elapsed: 0m 1s (remain 252m 50s) Max mem: 242 MB\n",
      "Epoch: [4/8][2500/13125] Loss: 0.3564(0.4124) Grad: 0.2472  LR: 0.000346  Elapsed: 2m 55s (remain 12m 25s) Max mem: 242 MB\n",
      "Epoch: [4/8][5000/13125] Loss: 0.5982(0.4118) Grad: 0.3169  LR: 0.000346  Elapsed: 5m 50s (remain 9m 29s) Max mem: 242 MB\n",
      "Epoch: [4/8][7500/13125] Loss: 0.4535(0.4128) Grad: 0.1542  LR: 0.000346  Elapsed: 8m 45s (remain 6m 34s) Max mem: 242 MB\n",
      "Epoch: [4/8][10000/13125] Loss: 0.4484(0.4122) Grad: 0.2299  LR: 0.000346  Elapsed: 11m 40s (remain 3m 38s) Max mem: 242 MB\n",
      "Epoch: [4/8][12500/13125] Loss: 0.4614(0.4122) Grad: 0.1456  LR: 0.000346  Elapsed: 14m 35s (remain 0m 43s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.243 (0.243) Elapsed 0m 0s (remain 10m 42s) Loss: 0.4800(0.4800) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.4122  avg_val_loss: 0.4090  time: 1050s\n",
      "Epoch 4 - Score: 0.8731\n",
      "Epoch 4 - Save Best Score: 0.8731 Model\n",
      "Epoch 4 - Save Best Loss: 0.4090 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [5/8][0/13125] Loss: 0.4128(0.4128) Grad: 0.1599  LR: 0.000250  Elapsed: 0m 1s (remain 257m 26s) Max mem: 242 MB\n",
      "Epoch: [5/8][2500/13125] Loss: 0.3468(0.4066) Grad: 0.2380  LR: 0.000250  Elapsed: 2m 54s (remain 12m 21s) Max mem: 242 MB\n",
      "Epoch: [5/8][5000/13125] Loss: 0.3187(0.4068) Grad: 0.2384  LR: 0.000250  Elapsed: 5m 47s (remain 9m 24s) Max mem: 242 MB\n",
      "Epoch: [5/8][7500/13125] Loss: 0.3592(0.4077) Grad: 0.1723  LR: 0.000250  Elapsed: 8m 40s (remain 6m 30s) Max mem: 242 MB\n",
      "Epoch: [5/8][10000/13125] Loss: 0.3807(0.4069) Grad: 0.1399  LR: 0.000250  Elapsed: 11m 34s (remain 3m 36s) Max mem: 242 MB\n",
      "Epoch: [5/8][12500/13125] Loss: 0.3000(0.4068) Grad: 0.2136  LR: 0.000250  Elapsed: 14m 28s (remain 0m 43s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.191 (0.191) Elapsed 0m 0s (remain 8m 43s) Loss: 0.4706(0.4706) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.4068  avg_val_loss: 0.4072  time: 1042s\n",
      "Epoch 5 - Score: 0.8742\n",
      "Epoch 5 - Save Best Score: 0.8742 Model\n",
      "Epoch 5 - Save Best Loss: 0.4072 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [6/8][0/13125] Loss: 0.4495(0.4495) Grad: 0.1893  LR: 0.000154  Elapsed: 0m 1s (remain 245m 1s) Max mem: 242 MB\n",
      "Epoch: [6/8][2500/13125] Loss: 0.3545(0.4018) Grad: 0.1310  LR: 0.000154  Elapsed: 2m 55s (remain 12m 24s) Max mem: 242 MB\n",
      "Epoch: [6/8][5000/13125] Loss: 0.4513(0.4024) Grad: 0.2595  LR: 0.000154  Elapsed: 5m 49s (remain 9m 28s) Max mem: 242 MB\n",
      "Epoch: [6/8][7500/13125] Loss: 0.4299(0.4033) Grad: 0.1758  LR: 0.000154  Elapsed: 8m 44s (remain 6m 33s) Max mem: 242 MB\n",
      "Epoch: [6/8][10000/13125] Loss: 0.2992(0.4022) Grad: 0.2218  LR: 0.000154  Elapsed: 11m 40s (remain 3m 38s) Max mem: 242 MB\n",
      "Epoch: [6/8][12500/13125] Loss: 0.3945(0.4023) Grad: 0.2052  LR: 0.000154  Elapsed: 14m 35s (remain 0m 43s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.184 (0.184) Elapsed 0m 0s (remain 8m 40s) Loss: 0.4535(0.4535) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.4022  avg_val_loss: 0.4063  time: 1050s\n",
      "Epoch 6 - Score: 0.8748\n",
      "Epoch 6 - Save Best Score: 0.8748 Model\n",
      "Epoch 6 - Save Best Loss: 0.4063 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [7/8][0/13125] Loss: 0.3675(0.3675) Grad: 0.1805  LR: 0.000073  Elapsed: 0m 1s (remain 243m 49s) Max mem: 242 MB\n",
      "Epoch: [7/8][2500/13125] Loss: 0.4381(0.3990) Grad: 0.1733  LR: 0.000073  Elapsed: 2m 52s (remain 12m 13s) Max mem: 242 MB\n",
      "Epoch: [7/8][5000/13125] Loss: 0.3268(0.3992) Grad: 0.2063  LR: 0.000073  Elapsed: 5m 47s (remain 9m 24s) Max mem: 242 MB\n",
      "Epoch: [7/8][7500/13125] Loss: 0.3865(0.3999) Grad: 0.2309  LR: 0.000073  Elapsed: 8m 42s (remain 6m 31s) Max mem: 242 MB\n",
      "Epoch: [7/8][10000/13125] Loss: 0.4835(0.3991) Grad: 0.2940  LR: 0.000073  Elapsed: 11m 36s (remain 3m 37s) Max mem: 242 MB\n",
      "Epoch: [7/8][12500/13125] Loss: 0.4341(0.3989) Grad: 0.1591  LR: 0.000073  Elapsed: 14m 29s (remain 0m 43s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.178 (0.178) Elapsed 0m 0s (remain 8m 24s) Loss: 0.4787(0.4787) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.3987  avg_val_loss: 0.4072  time: 1042s\n",
      "Epoch 7 - Score: 0.8750\n",
      "Epoch 7 - Save Best Score: 0.8750 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch: [8/8][0/13125] Loss: 0.3528(0.3528) Grad: 0.1948  LR: 0.000019  Elapsed: 0m 1s (remain 245m 23s) Max mem: 242 MB\n",
      "Epoch: [8/8][2500/13125] Loss: 0.4338(0.3965) Grad: 0.1706  LR: 0.000019  Elapsed: 2m 55s (remain 12m 24s) Max mem: 242 MB\n",
      "Epoch: [8/8][5000/13125] Loss: 0.4734(0.3967) Grad: 0.2114  LR: 0.000019  Elapsed: 5m 49s (remain 9m 27s) Max mem: 242 MB\n",
      "Epoch: [8/8][7500/13125] Loss: 0.3549(0.3977) Grad: 0.1874  LR: 0.000019  Elapsed: 8m 44s (remain 6m 33s) Max mem: 242 MB\n",
      "Epoch: [8/8][10000/13125] Loss: 0.4128(0.3968) Grad: 0.2423  LR: 0.000019  Elapsed: 11m 38s (remain 3m 38s) Max mem: 242 MB\n",
      "Epoch: [8/8][12500/13125] Loss: 0.4951(0.3965) Grad: 0.2529  LR: 0.000019  Elapsed: 14m 33s (remain 0m 43s) Max mem: 242 MB\n",
      "EVAL: [0/2188] Data 0.147 (0.147) Elapsed 0m 0s (remain 7m 10s) Loss: 0.4692(0.4692) \n"
     ]
    }
   ],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df['preds'].values\n",
    "    labels = result_df[CFG.target_col].values\n",
    "    score = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}')\n",
    "\n",
    "if CFG.train:\n",
    "    # train \n",
    "    oof_df = pd.DataFrame()\n",
    "    kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "\n",
    "    folds = list(kf.split(all_files))\n",
    "    for fold in range(CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            trn_idx, val_idx = folds[fold]\n",
    "            train_files = all_files[trn_idx]\n",
    "            valid_files = all_files[val_idx]\n",
    "            _oof_df = train_loop(train_files, valid_files, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    # save result\n",
    "    oof_df.to_csv(SAVEDIR / 'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for fold  in CFG.trn_fold:\n",
    "    states.append(torch.load(os.path.join(SAVEDIR, f'{CFG.model_name}_fold{fold}_best_score.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T14:19:43.733214Z",
     "iopub.status.busy": "2021-09-15T14:19:43.732832Z",
     "iopub.status.idle": "2021-09-15T14:19:44.708852Z",
     "shell.execute_reply": "2021-09-15T14:19:44.70795Z",
     "shell.execute_reply.started": "2021-09-15T14:19:43.733182Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f\"test\"\n",
    "            \n",
    "all_files = []\n",
    "all_files.extend(np.sort(np.array(tf.io.gfile.glob(path + \"/test*.tfrecords\"))))\n",
    "    \n",
    "print(\"test_files: \", len(all_files))\n",
    "all_files = np.array(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:31:55.957569Z",
     "iopub.status.busy": "2021-09-15T15:31:55.957245Z",
     "iopub.status.idle": "2021-09-15T15:32:00.575684Z",
     "shell.execute_reply": "2021-09-15T15:32:00.574835Z",
     "shell.execute_reply.started": "2021-09-15T15:31:55.957542Z"
    }
   },
   "outputs": [],
   "source": [
    "model= CNN1d()\n",
    "model.to(device)\n",
    "\n",
    "wave_ids = []\n",
    "probs_all = []\n",
    "\n",
    "for fold, state in enumerate(states):\n",
    "\n",
    "    model.load_state_dict(state['model'])\n",
    "    model.eval()\n",
    "    probs = []\n",
    "\n",
    "    test_loader = TFRecordDataLoader(all_files, batch_size=CFG.val_batch_size, \n",
    "                                     shuffle=False, labeled=False)\n",
    "\n",
    "    for i, d in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        x = bandpass(d[0], **CFG.bandpass_params)\n",
    "        x = torch.from_numpy(x).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(x)\n",
    "        preds = y_preds.sigmoid().to('cpu').numpy()\n",
    "        probs.append(preds)\n",
    "\n",
    "        if fold==0: # same test loader, no need to do this the second time\n",
    "            wave_ids.append(d[1].astype('U13'))\n",
    "\n",
    "    probs = np.concatenate(probs)\n",
    "    probs_all.append(probs)\n",
    "\n",
    "probs_avg = np.asarray(probs_all).mean(axis=0).flatten()\n",
    "wave_ids = np.concatenate(wave_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:36:14.629455Z",
     "iopub.status.busy": "2021-09-15T15:36:14.629104Z",
     "iopub.status.idle": "2021-09-15T15:36:14.644987Z",
     "shell.execute_reply": "2021-09-15T15:36:14.644195Z",
     "shell.execute_reply.started": "2021-09-15T15:36:14.629422Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({'id': wave_ids, 'target': probs_avg})\n",
    "# Save test dataframe to disk\n",
    "folds = '_'.join([str(s) for s in CFG.trn_fold])\n",
    "test_df.to_csv(f'{CFG.model_name}_folds_{folds}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo shutdown -h now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
